{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epqx-zo040z6",
        "outputId": "deda8052-f22a-4000-dbc1-b38b1d361fec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import timeit\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "# !pip install catboost\n",
        "# !pip install scikit-learn\n",
        "\n",
        "# !pip install scikit-plot\n",
        "\n",
        "from scikitplot.metrics import plot_confusion_matrix\n",
        "\n",
        "import catboost as cb\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from imblearn.under_sampling import CondensedNearestNeighbour\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, zero_one_loss\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from termcolor import colored\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install scikit-plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFZumaaj6-Nl",
        "outputId": "b93c1cdc-fe8e-4a08-8bba-51ce39b3612b"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.random.seed(100)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "dataset_root = './'\n",
        "\n",
        "train_file = os.path.join(dataset_root, 'KDDTrain+.txt')\n",
        "test_file = os.path.join(dataset_root, 'KDDTest+.txt')\n",
        "\n",
        "# Original KDD dataset feature names obtained from\n",
        "# http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
        "# http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
        "\n",
        "header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
        "                'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
        "                'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n",
        "                'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
        "                'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
        "                'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "                'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type',\n",
        "                'success_pred']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "w-76Pm478ZL5"
      },
      "outputs": [],
      "source": [
        "# Differentiating between nominal, binary, and numeric features\n",
        "\n",
        "# root_shell is marked as a continuous feature in the kddcup.names\n",
        "# file, but it is supposed to be a binary feature according to the\n",
        "# dataset documentation\n",
        "\n",
        "col_names = np.array(header_names)\n",
        "\n",
        "nominal_idx = [1, 2, 3]\n",
        "binary_idx = [6, 11, 13, 14, 20, 21]\n",
        "numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx))\n",
        "\n",
        "nominal_cols = col_names[nominal_idx].tolist()\n",
        "binary_cols = col_names[binary_idx].tolist()\n",
        "numeric_cols = col_names[numeric_idx].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "kpKnil7N8l8d",
        "outputId": "f2ab7063-efcb-4f29-8867-d2a298969b89"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'KDDTrain+.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/danya/Desktop/Danya/NSP/dataaug.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         category[cat]\u001b[39m.\u001b[39mappend(attack)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m attack_mapping \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((v, k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m category \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m category[k])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mKDDTrain+.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, names\u001b[39m=\u001b[39;49mheader_names)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m train_df[\u001b[39m'\u001b[39m\u001b[39mattack_category\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mattack_type\u001b[39m\u001b[39m'\u001b[39m] \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: attack_mapping[x])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m train_df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39msuccess_pred\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/nsp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[0;32m~/miniconda3/envs/nsp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/miniconda3/envs/nsp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[0;32m~/miniconda3/envs/nsp/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/miniconda3/envs/nsp/lib/python3.9/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KDDTrain+.txt'"
          ]
        }
      ],
      "source": [
        "# training_attack_types.txt maps each of the 22 different attacks to 1 of 4 categories\n",
        "# file obtained from http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types\n",
        "\n",
        "\n",
        "category = defaultdict(list)\n",
        "category['benign'].append('normal')\n",
        "\n",
        "with open('./training_attack_types.txt', 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        attack, cat = line.strip().split(' ')\n",
        "        category[cat].append(attack)\n",
        "\n",
        "attack_mapping = dict((v, k) for k in category for v in category[k])\n",
        "/Users/danya/Desktop/Danya/NSP/dataaug.ipynb\n",
        "train_df = pd.read_csv(\"./\", names=header_names)\n",
        "\n",
        "train_df['attack_category'] = train_df['attack_type'] \\\n",
        "    .map(lambda x: attack_mapping[x])\n",
        "train_df.drop(['success_pred'], axis=1, inplace=True)\n",
        "\n",
        "test_df = pd.read_csv(test_file, names=header_names)\n",
        "test_df['attack_category'] = test_df['attack_type'] \\\n",
        "    .map(lambda x: attack_mapping[x])\n",
        "test_df.drop(['success_pred'], axis=1, inplace=True)\n",
        "\n",
        "train_attack_types = train_df['attack_type'].value_counts()\n",
        "train_attack_cats = train_df['attack_category'].value_counts()\n",
        "\n",
        "test_attack_types = test_df['attack_type'].value_counts()\n",
        "test_attack_cats = test_df['attack_category'].value_counts()\n",
        "\n",
        "train_attack_types.plot(kind='barh', figsize=(20, 10), fontsize=20)\n",
        "\n",
        "train_attack_cats.plot(kind='barh', figsize=(20, 10), fontsize=30)\n",
        "\n",
        "test_attack_types.plot(kind='barh', figsize=(20, 10), fontsize=15)\n",
        "\n",
        "test_attack_cats.plot(kind='barh', figsize=(20, 10), fontsize=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p25TZIAc8xYT",
        "outputId": "1e4cdfd6-f2b7-40f6-fb1e-b735240c1c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['duration', 'protocol_type', 'service', 'flag', 'src_bytes',\n",
            "       'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
            "       'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell',\n",
            "       'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
            "       'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
            "       'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
            "       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
            "       'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
            "       'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
            "       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
            "       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
            "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
            "       'dst_host_srv_rerror_rate', 'attack_type', 'attack_category'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Let's take a look at the binary features\n",
        "# By definition, all of these features should have a min of 0.0 and a max of 1.0\n",
        "# execute the commands in console\n",
        "\n",
        "train_df[binary_cols].describe().transpose()\n",
        "\n",
        "# Wait a minute... the su_attempted column has a max value of 2.0?\n",
        "\n",
        "train_df.groupby(['su_attempted']).size()\n",
        "\n",
        "# Let's fix this discrepancy and assume that su_attempted=2 -> su_attempted=0\n",
        "\n",
        "train_df['su_attempted'].replace(2, 0, inplace=True)\n",
        "test_df['su_attempted'].replace(2, 0, inplace=True)\n",
        "train_df.groupby(['su_attempted']).size()\n",
        "# Next, we notice that the num_outbound_cmds column only takes on one value!\n",
        "print(train_df.columns)\n",
        "\n",
        "\n",
        "# train_df.groupby(['num_outbound_cmds']).size()\n",
        "\n",
        "# # Now, that's not a very useful feature - let's drop it from the dataset\n",
        "\n",
        "# train_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
        "# test_df.drop('num_outbound_cmds', axis=1, inplace=True)\n",
        "# numeric_cols.remove('num_outbound_cmds')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IPOFfmlD87Oa",
        "outputId": "6f2fe78b-6908-4a07-c48b-30c0796f8040"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# transform the dataset\\noversample = SMOTE()\\ntrain_x, train_Y = oversample.fit_resample(train_x, train_Y)'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Data Preparation\n",
        "\n",
        "\"\"\"\n",
        "train_Y = train_df['attack_category']\n",
        "train_x_raw = train_df.drop(['attack_category', 'attack_type'], axis=1)\n",
        "test_Y = test_df['attack_category']\n",
        "test_x_raw = test_df.drop(['attack_category', 'attack_type'], axis=1)\n",
        "\n",
        "'''# feature selection\n",
        "def select_features(X_train, y_train, X_test):\n",
        "    fs = SelectKBest(score_func=mutual_info_classif, k=30)\n",
        "    fs.fit(X_train, y_train)\n",
        "    X_train_fs = fs.transform(X_train)\n",
        "    X_test_fs = fs.transform(X_test)\n",
        "    return X_train_fs, X_test_fs\n",
        "'''\n",
        "\n",
        "combined_df_raw = pd.concat([train_x_raw, test_x_raw])\n",
        "combined_df = pd.get_dummies(combined_df_raw, columns=nominal_cols, drop_first=True)\n",
        "\n",
        "train_x = combined_df[:len(train_x_raw)]\n",
        "test_x = combined_df[len(train_x_raw):]\n",
        "\n",
        "# use this for catboost\n",
        "x_train = train_x_raw\n",
        "x_test = test_x_raw\n",
        "\n",
        "# Store dummy variable feature names\n",
        "dummy_variables = list(set(train_x) - set(combined_df_raw))\n",
        "\n",
        "# execute the commands in console\n",
        "train_x.describe()\n",
        "train_x['duration'].describe()\n",
        "\n",
        "# Experimenting with StandardScaler on the single 'duration' feature\n",
        "durations = train_x['duration'].values.reshape(-1, 1)\n",
        "standard_scaler = StandardScaler().fit(durations)\n",
        "scaled_durations = standard_scaler.transform(durations)\n",
        "pd.Series(scaled_durations.flatten()).describe()\n",
        "\n",
        "# Experimenting with MinMaxScaler on the single 'duration' feature\n",
        "\n",
        "min_max_scaler = MinMaxScaler().fit(durations)\n",
        "min_max_scaled_durations = min_max_scaler.transform(durations)\n",
        "pd.Series(min_max_scaled_durations.flatten()).describe()\n",
        "\n",
        "# Experimenting with RobustScaler on the single 'duration' feature\n",
        "\n",
        "min_max_scaler = RobustScaler().fit(durations)\n",
        "robust_scaled_durations = min_max_scaler.transform(durations)\n",
        "pd.Series(robust_scaled_durations.flatten()).describe()\n",
        "\n",
        "# Let's proceed with StandardScaler- Apply to all the numeric columns\n",
        "\n",
        "standard_scaler = StandardScaler().fit(train_x[numeric_cols])\n",
        "\n",
        "train_x[numeric_cols] = \\\n",
        "    standard_scaler.transform(train_x[numeric_cols])\n",
        "\n",
        "test_x[numeric_cols] = \\\n",
        "    standard_scaler.transform(test_x[numeric_cols])\n",
        "\n",
        "train_x.describe()\n",
        "\n",
        "train_Y_bin = train_Y.apply(lambda x: 0 if x is 'benign' else 1)\n",
        "test_Y_bin = test_Y.apply(lambda x: 0 if x is 'benign' else 1)\n",
        "\n",
        "'''# transform the dataset\n",
        "oversample = SMOTE()\n",
        "train_x, train_Y = oversample.fit_resample(train_x, train_Y)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_x' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/danya/Desktop/Danya/NSP/dataaug.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m oversample \u001b[39m=\u001b[39m SMOTE(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danya/Desktop/Danya/NSP/dataaug.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_x, train_Y \u001b[39m=\u001b[39m oversample\u001b[39m.\u001b[39mfit_resample(train_x, train_Y)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
          ]
        }
      ],
      "source": [
        "oversample = SMOTE(random_state=42)\n",
        "train_x, train_Y = oversample.fit_resample(train_x, train_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SMOTE' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/danya/Downloads/nsp_attack_classification_1.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danya/Downloads/nsp_attack_classification_1.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m attack_samples \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m67343\u001b[39m \u001b[39m*\u001b[39m \u001b[39m0.3\u001b[39m) \u001b[39m# 1.5 times the majority class to make 'attack' 60%\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danya/Downloads/nsp_attack_classification_1.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m oversample \u001b[39m=\u001b[39m SMOTE(sampling_strategy\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mdos\u001b[39m\u001b[39m'\u001b[39m: attack_samples, \u001b[39m'\u001b[39m\u001b[39mprobe\u001b[39m\u001b[39m'\u001b[39m: attack_samples, \u001b[39m'\u001b[39m\u001b[39mr2l\u001b[39m\u001b[39m'\u001b[39m: attack_samples, \u001b[39m'\u001b[39m\u001b[39mu2r\u001b[39m\u001b[39m'\u001b[39m: attack_samples  }, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danya/Downloads/nsp_attack_classification_1.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_x, train_Y \u001b[39m=\u001b[39m oversample\u001b[39m.\u001b[39mfit_resample(train_x, train_Y)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
          ]
        }
      ],
      "source": [
        "#attack_samples = int(67343 * 1.5) # 1.5 times the majority class to make 'attack' 60%\n",
        "\n",
        "#sampling_strategy={'dos': attack_samples, 'probe': attack_samples, 'r2l': attack_samples, 'u2r': attack_samples  }, \n",
        "oversample = SMOTE(random_state=42)\n",
        "train_x, train_Y = oversample.fit_resample(train_x, train_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attack_category\n",
            "attack    404056\n",
            "benign    101014\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "oversample = SMOTE(random_state=42)\n",
        "train_x, train_Y = oversample.fit_resample(train_x, train_Y)\n",
        "\n",
        "# Convert y_resampled to a DataFrame\n",
        "train_Y_df = pd.DataFrame(train_Y, columns=['attack_category'])  # assuming 'target' is your label column\n",
        "\n",
        "# Combine all attack types into one category\n",
        "train_Y_df['attack_category'] = train_Y_df['attack_category'].map(lambda x: 'benign' if x == 'benign' else 'attack')\n",
        "\n",
        "# Print the new class distribution\n",
        "print(train_Y_df['attack_category'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oversample = SMOTE(sampling_strategy={'dos': attack_samples, 'probe': attack_samples, 'r2l': attack_samples, 'u2r': attack_samples  }, random_state=42)\n",
        "train_x, train_Y = oversample.fit_resample(train_x, train_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['benign' 101014]\n",
            " ['dos' 101014]\n",
            " ['probe' 101014]\n",
            " ['r2l' 101014]\n",
            " ['u2r' 101014]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "unique, counts = np.unique(train_Y, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class=benign, n=101014 (20.000%)\n",
            "Class=dos, n=101014 (20.000%)\n",
            "Class=r2l, n=101014 (20.000%)\n",
            "Class=probe, n=101014 (20.000%)\n",
            "Class=u2r, n=101014 (20.000%)\n"
          ]
        }
      ],
      "source": [
        "counter = Counter(train_Y)\n",
        "\n",
        "# Print the class distribution\n",
        "for k,v in counter.items():\n",
        "    per = v / len(train_Y) * 100\n",
        "    print('Class=%s, n=%d (%.3f%%)' % (k, v, per))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "X_augmented_df = pd.DataFrame(train_x, columns=train_x.columns)\n",
        "y_augmented_df = pd.DataFrame(train_Y, columns=['attack_category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_augmented_df.to_csv('X_augmented.csv', index=False)\n",
        "y_augmented_df.to_csv('y_augmented.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "augmenented_df = pd.concat([X_augmented_df, y_augmented_df], axis=1)\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "augmenented_df.to_csv('augmenented_df.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SqUCFVu5LgY"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
